# -*- coding: utf-8 -*-
"""NER-2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcLfACPHWbPzZJjMJZW4v6ExVXU5pP0i
"""

!pip install -q datasets evaluate gradio transformers seqeval

import os
import pandas as pd
import numpy as np
import torch
import evaluate
import gradio as gr
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer
)

# âœ… Load dataset
df = pd.read_csv("/content/legal_ner_dataset_entity_per_row (1).csv")
print("âœ… Dataset Loaded")

# âœ… Tokenizer and labels
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
unique_labels = sorted(df["label"].dropna().unique())
label_list = ["O"] + [f"B-{l}" for l in unique_labels] + [f"I-{l}" for l in unique_labels]
label_to_id = {l: i for i, l in enumerate(label_list)}
id_to_label = {i: l for l, i in label_to_id.items()}

# âœ… Label aligner
def align_labels_with_tokens(sentence, entities):
    encoding = tokenizer(sentence, return_offsets_mapping=True, truncation=True, add_special_tokens=False)
    labels = ["O"] * len(encoding["offset_mapping"])

    for ent_text, ent_type in entities:
        start = sentence.find(ent_text)
        if start == -1:
            continue
        end = start + len(ent_text)
        for i, (token_start, token_end) in enumerate(encoding["offset_mapping"]):
            if token_start >= start and token_end <= end:
                if token_start == start:
                    labels[i] = f"B-{ent_type}"
                else:
                    labels[i] = f"I-{ent_type}"

    tokens = tokenizer.convert_ids_to_tokens(encoding["input_ids"])
    label_ids = [label_to_id[l] for l in labels]
    return tokens, label_ids

formatted = []
for sentence in df["sentence"].unique():
    entities = df[df["sentence"] == sentence][["entity", "label"]].values.tolist()
    tokens, tags = align_labels_with_tokens(sentence, entities)
    formatted.append({"tokens": tokens, "ner_tags": tags})

dataset = Dataset.from_list(formatted).train_test_split(test_size=0.3)

# âœ… Tokenization
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=64
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)

# âœ… Load model
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", num_labels=len(label_list))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# âœ… Trainer setup
args = TrainingArguments(
    output_dir="legal-ner-bert",
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="no",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.03,
    report_to="none"


)

metric = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)
    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]
    true_preds = [[id_to_label[p] for (p, l) in zip(pred, label) if l != -100]
                  for pred, label in zip(predictions, labels)]
    results = metric.compute(predictions=true_preds, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"]
    }

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset["train"].remove_columns(["tokens", "ner_tags"]),
    eval_dataset=tokenized_dataset["test"].remove_columns(["tokens", "ner_tags"]),
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# âœ… Train model
trainer.train()

# âœ… Inference function with CLS/SEP removed
def ner_infer(text):
    try:
        # Get offset mapping separately (not passed to model)
        encoding = tokenizer(text, return_offsets_mapping=True, truncation=True)
        offsets = encoding["offset_mapping"]

        # Get model inputs
        inputs = tokenizer(text, return_tensors="pt", truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Predict
        with torch.no_grad():
            logits = model(**inputs).logits

        preds = torch.argmax(logits, dim=2)[0].cpu().numpy()
        input_ids = inputs["input_ids"][0]
        tokens = tokenizer.convert_ids_to_tokens(input_ids)

        result = []
        current_entity = ""
        current_label = ""

        for token, pred in zip(tokens, preds):
            if token in ["[CLS]", "[SEP]"]:
                continue  # skip special tokens

            label = id_to_label[pred]
            word = token.replace("##", "")

            if label == "O":
                if current_entity:
                    result.append(f"[{current_entity.strip()} â†’ {current_label}]")
                    current_entity = ""
                    current_label = ""
                result.append(word)
            elif label.startswith("B-"):
                if current_entity:
                    result.append(f"[{current_entity.strip()} â†’ {current_label}]")
                current_entity = word + " "
                current_label = label[2:]
            elif label.startswith("I-") and current_label == label[2:]:
                current_entity += word + " "
            else:
                if current_entity:
                    result.append(f"[{current_entity.strip()} â†’ {current_label}]")
                result.append(word)
                current_entity = ""
                current_label = ""

        if current_entity:
            result.append(f"[{current_entity.strip()} â†’ {current_label}]")

        return " ".join(result)

    except Exception as e:
        return f"Error: {str(e)}"

# âœ… Gradio UI
gr.Interface(
    fn=ner_infer,
    inputs=gr.Textbox(lines=4, placeholder="Enter a legal sentence here..."),
    outputs="text",
    title="ğŸ“œ Legal Entity Extractor (BERT-Base-Cased)",
    description="Extracts PERSON, ORG, DATE, LAW from legal text using fine-tuned BERT."
).launch()

# âœ… Gradio UI
gr.Interface(
    fn=ner_infer,
    inputs=gr.Textbox(lines=4, placeholder="Enter a legal sentence here..."),
    outputs="text",
    title="ğŸ“œ Legal Entity Extractor (BERT-Base-Cased)",
    description="Extracts PERSON, ORG, DATE, LAW from legal text using fine-tuned BERT."
).launch()